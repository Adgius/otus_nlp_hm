{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poUsaUe7nDEO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/RussianNLP/RuCoLA.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "YVH13UlPOX18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import get_scheduler\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "IX-tFfpfGi8T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/RuCoLA/data/in_domain_train.csv\", usecols=[1,2])\n",
        "test = pd.read_csv(\"/content/RuCoLA/data/in_domain_dev.csv\", usecols=[1,2])\n",
        "\n",
        "idx = train.sample(frac=0.8, random_state=123).index\n",
        "val = train[~train.index.isin(idx)]\n",
        "train = train[train.index.isin(idx)]\n",
        "\n",
        "del idx\n",
        "\n",
        "print('Train size:', len(train))\n",
        "print('Val size:', len(val))\n",
        "print('Test size:', len(test))\n",
        "print('\\n')\n",
        "print('Train labels counts\\n', train['acceptable'].value_counts().to_dict(), '\\n')\n",
        "print('Eval labels counts\\n', val['acceptable'].value_counts().to_dict(), '\\n')\n",
        "print('Test labels counts\\n', test['acceptable'].value_counts().to_dict(), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHLKLjxwGMYT",
        "outputId": "2247dd12-97a7-4cfc-9d37-03e99973b4ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 6295\n",
            "Val size: 1574\n",
            "Test size: 983\n",
            "\n",
            "\n",
            "Train labels counts\n",
            " {1: 4704, 0: 1591} \n",
            "\n",
            "Eval labels counts\n",
            " {1: 1160, 0: 414} \n",
            "\n",
            "Test labels counts\n",
            " {1: 733, 0: 250} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "hb9r96-efiHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ai-forever/ruBert-base\")"
      ],
      "metadata": {
        "id": "OjjS90D9OMeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in train['sentence']:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmC6vzpPQW1U",
        "outputId": "a10db44a-efa0-48ac-9048-1b5162e5f2a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "2AW9ewEHSWuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356f9308-10fc-418c-f6b9-5445fdf9e8e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EvalDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, X):\n",
        "        self.text = X.reset_index(drop=True)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=45)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.text.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        output = self.text[index]\n",
        "        output = self.tokenize(output)\n",
        "        return {k: v.reshape(-1).to(device) for k, v in output.items()}\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, X, label):\n",
        "        self.text = X.reset_index(drop=True)\n",
        "        self.label = label.reset_index(drop=True)\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        return tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=45)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.label.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        output = self.text[index]\n",
        "        output = self.tokenize(output)\n",
        "        output.update({'labels': torch.tensor(self.label[index])})\n",
        "        return {k: v.reshape(-1).to(device) for k, v in output.items()}\n",
        "        \n",
        "\n",
        "train_ds = TrainDataset(train['sentence'], train['acceptable'])\n",
        "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "\n",
        "eval_ds = TrainDataset(val['sentence'], val['acceptable'])\n",
        "eval_dataloader = DataLoader(eval_ds, batch_size=32)\n",
        "\n",
        "test_ds = EvalDataset(test['sentence'])\n",
        "test_dataloader = DataLoader(test_ds, batch_size=32)"
      ],
      "metadata": {
        "id": "ZUq-6bKaGuRj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=5e-6)\n",
        "\n",
        "num_epochs = 2\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "uA1Vns7HRx62"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dataloader, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1} \\n -------------------')\n",
        "        for n_batch, batch in enumerate(train_dataloader):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            if n_batch % 50 == 0:\n",
        "                loss_value, current = loss.item(), n_batch * batch['input_ids'].shape[0]\n",
        "                print(f\"Loss train: {loss_value:>7f}  [{current:>5d}/{len(train_ds):>5d}]\")\n",
        "                print('Evaluating...')\n",
        "                preds, true = test_model(eval_dataloader, eval=True)\n",
        "                print(f'F1-score = {f1_score(preds, true):>3f}\\n')\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad() \n",
        "\n",
        "def test_model(test_dataloader, eval=False):\n",
        "    model.eval()\n",
        "    y_pred = np.array([])\n",
        "    y_true = np.array([])\n",
        "    for n_batch, batch in enumerate(test_dataloader):\n",
        "        if eval:\n",
        "            y_true = np.hstack([y_true, batch['labels'].cpu().numpy().reshape(-1)])\n",
        "        outputs = model(**batch)\n",
        "        y_pred = np.hstack([y_pred, outputs['logits'].argmax(axis=1).detach().cpu().numpy()])\n",
        "    return y_pred, y_true"
      ],
      "metadata": {
        "id": "Ro7LMcI-U9nh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_dataloader, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4DSUuoA3Uea",
        "outputId": "94653b2a-b6bf-4d37-eaf5-a8d560ad2eab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            " -------------------\n",
            "Loss train: 0.798126  [    0/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.174248\n",
            "\n",
            "Loss train: 0.528468  [ 1600/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.848574\n",
            "\n",
            "Loss train: 0.456054  [ 3200/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.847619\n",
            "\n",
            "Loss train: 0.654314  [ 4800/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.848863\n",
            "\n",
            "Epoch 2 \n",
            " -------------------\n",
            "Loss train: 0.429925  [    0/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.848889\n",
            "\n",
            "Loss train: 0.675536  [ 1600/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.851190\n",
            "\n",
            "Loss train: 0.416684  [ 3200/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.852324\n",
            "\n",
            "Loss train: 0.529038  [ 4800/ 6295]\n",
            "Evaluating...\n",
            "F1-score = 0.853503\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, _ = test_model(test_dataloader, eval=False)\n",
        "print(f'F1-score = {f1_score(y_pred, test[\"acceptable\"]):>3f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xyHHcE59jwK",
        "outputId": "0cd4787f-3870-4021-a4fb-1c758b207823"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.861945\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RuGPT3"
      ],
      "metadata": {
        "id": "YEabO-b2foKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/rugpt3large_based_on_gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3large_based_on_gpt2\")"
      ],
      "metadata": {
        "id": "Wq7GZXRJfuwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "S944vZ6LKRuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(text):\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer.encode(text, return_tensors='pt').reshape(-1).to(device)\n",
        "        loss = model(input_ids=inputs, labels=inputs).loss.item()\n",
        "        return loss\n",
        "calc_loss('Предложение корректное?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHtLa1rfbZJx",
        "outputId": "570a6101-7155-4dd9-fe64-11314bf296ae"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.507739067077637"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero shot"
      ],
      "metadata": {
        "id": "FSYKGaURQpnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#zero shot\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "def shot(start: str, text: str, end: list):\n",
        "    first = ' '.join([start, text, end[0]])\n",
        "    second = ' '.join([start, text, end[1]])\n",
        "\n",
        "    loss_1, loss_2 =  calc_loss(first), calc_loss(second)\n",
        "    return 1 if loss_1 > loss_2 else 0\n",
        "\n",
        "y_pred = test['sentence'].progress_apply(lambda x: shot('Проверь корректность предложения.', x, ['Это предложение корректное.', 'Это предложение некорректно']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJXMKt8QL9Ii",
        "outputId": "a1863fd7-fdf2-4853-d19b-09d1b40b4ee2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 983/983 [01:42<00:00,  9.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'F1-score = {f1_score(y_pred, test[\"acceptable\"]):>3f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nui3BaSP9c3",
        "outputId": "007a2e92-dab7-484c-a6ac-10aad5d3efbd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.518062\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = test['sentence'].progress_apply(lambda x: shot('Если ли здесь ошибка?', x, ['Предложение правильное.', 'Допущена ошибка']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-1y5xsUOdLl",
        "outputId": "17ff69d8-5233-49f3-bd4d-40d756542c59"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 983/983 [01:07<00:00, 14.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.854810\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'F1-score = {f1_score(y_pred, test[\"acceptable\"]):>3f}\\n') # Какая большая разница с предыдущей затравкой"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqD-XhJMQZUn",
        "outputId": "f0bb75d2-9fb7-4cbd-d7fb-12ce1c5b4f16"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.854810\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few shot"
      ],
      "metadata": {
        "id": "CcMQVS2oQslA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 shots\n",
        "promt = \"\"\"Проверь корректность предложения:\n",
        "Вдруг решетка беззвучно поехала в сторону, и на балконе возникла таинственная фигура, прячущаяся от лунного света, и погрозила Ивану пальцем. => Верно\n",
        "Этим летом не никуда ездили. => Неверно\n",
        "\"\"\"\n",
        "y_pred = test['sentence'].apply(lambda x: shot(promt, x, ['=> Верно', '=> Неверно']))\n",
        "print(f'F1-score = {f1_score(y_pred, test[\"acceptable\"]):>3f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCFO2RxtSzYr",
        "outputId": "caf3ef58-871d-42ef-923c-4bcc7a720c03"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.802253\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 shots. Change promt\n",
        "promt = \"\"\"Проверь корректность предложения:\n",
        "Вдруг решетка беззвучно поехала в сторону, и на балконе возникла таинственная фигура, прячущаяся от лунного света, и погрозила Ивану пальцем. Предложение правильное.\n",
        "Этим летом не никуда ездили. Допущена ошибка\n",
        "\"\"\"\n",
        "y_pred = test['sentence'].apply(lambda x: shot(promt, x, ['Предложение правильное.', 'Допущена ошибка']))\n",
        "print(f'F1-score = {f1_score(y_pred, test[\"acceptable\"]):>3f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH98YyN4UkWg",
        "outputId": "162308b5-e28b-4ec7-a114-7b318e75afd3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.849589\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 shots\n",
        "promt = \"\"\"Проверь корректность предложения:\n",
        "Вдруг решетка беззвучно поехала в сторону, и на балконе возникла таинственная фигура, прячущаяся от лунного света, и погрозила Ивану пальцем. Предложение правильное.\n",
        "Этим летом не никуда ездили. Допущена ошибка\n",
        "На поверку вся теория оказалась полной чепухой. Предложение правильное.\n",
        "Симптомов болезни не исчезло. Допущена ошибка\n",
        "\"\"\"\n",
        "y_pred = test['sentence'].apply(lambda x: shot(promt, x, ['Предложение правильное.', 'Допущена ошибка']))\n",
        "print(f'F1-score = {f1_score(y_pred, test[\"acceptable\"]):>3f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8FYho_9U51M",
        "outputId": "fcb93619-e5fa-46cb-c727-a03f32604a88"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.820859\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Т5"
      ],
      "metadata": {
        "id": "TWfLaByKGddI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install --no-cache-dir transformers sentencepiece"
      ],
      "metadata": {
        "id": "xn_Fc4zxJGaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruT5-base\", use_fast=False)\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"ai-forever/ruT5-base\")"
      ],
      "metadata": {
        "id": "D5waU3ntGff3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "VOE0gkD-MKDw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvalDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, X):\n",
        "        self.text = X.reset_index(drop=True)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=45)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.text.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        output = self.text[index]\n",
        "        output = self.tokenize(output)\n",
        "        return {k: v.reshape(-1).to(device) for k, v in output.items()}\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, X, label):\n",
        "        self.text = X.reset_index(drop=True)\n",
        "        self.label = label.reset_index(drop=True)\n",
        "    \n",
        "    def tokenize(self, text, length=45):\n",
        "        return tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.label.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        output = self.text[index]\n",
        "        output = self.tokenize(output)\n",
        "        output = {k: v.reshape(-1).to(device) for k, v in output.items()}\n",
        "\n",
        "        label = 'верно' if self.label[index] == 1 else 'неверно'\n",
        "        label = self.tokenize(label, length=2).input_ids.reshape(-1).to(device)\n",
        "\n",
        "        output.update({'labels': label})\n",
        "        return output\n",
        "        \n",
        "train_ds = TrainDataset(train['sentence'], train['acceptable'])\n",
        "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "\n",
        "eval_ds = TrainDataset(val['sentence'], val['acceptable'])\n",
        "eval_dataloader = DataLoader(eval_ds, batch_size=32)\n",
        "\n",
        "test_ds = EvalDataset(test['sentence'])\n",
        "test_dataloader = DataLoader(test_ds, batch_size=32)"
      ],
      "metadata": {
        "id": "Zj4_AcNuHBnp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "num_epochs = 2\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "KjizKXr5A7WF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dataloader, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1} \\n -------------------')\n",
        "        for n_batch, batch in enumerate(train_dataloader):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            if n_batch % 50 == 0:\n",
        "                loss_train, current = loss.item(), n_batch * batch['input_ids'].shape[0]\n",
        "                print(f\"Loss train: {loss_train:>7f}  [{current:>5d}/{len(train_ds):>5d}]\")\n",
        "                print('Evaluating...')\n",
        "                loss_val, _ = test_model(eval_dataloader, eval=True)\n",
        "                print(f\"Loss test: {loss_val:>7f}\\n\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad() \n",
        "\n",
        "def test_model(test_dataloader, eval=False):\n",
        "    model.eval()\n",
        "    y_pred = np.array([])\n",
        "    y_true = np.array([])\n",
        "    loss = []\n",
        "    for n_batch, batch in enumerate(test_dataloader):\n",
        "        if not eval:\n",
        "            gen_tok = model.generate(**batch)\n",
        "            gen_tok = [1 if 2937 in i else 0 for i in gen_tok]  # tokenizer.decode(2937) == 'верно'\n",
        "            y_true = np.hstack([y_true, gen_tok])\n",
        "        else:\n",
        "            outputs = model(**batch)\n",
        "            loss.append(outputs.loss.item())\n",
        "    if not eval:\n",
        "        return y_true\n",
        "    else:\n",
        "        return np.sum(loss)/len(loss), y_true"
      ],
      "metadata": {
        "id": "KCUX0SasA-Cq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_dataloader, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxLrAPTaZVWM",
        "outputId": "ef5d1257-a597-4a2f-ccc0-d296b51db464"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            " -------------------\n",
            "Loss train: 13.994221  [    0/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 16.116276\n",
            "\n",
            "Loss train: 0.341062  [ 1600/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 0.341219\n",
            "\n",
            "Loss train: 0.284652  [ 3200/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 0.312012\n",
            "\n",
            "Loss train: 0.312784  [ 4800/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 0.305643\n",
            "\n",
            "Epoch 2 \n",
            " -------------------\n",
            "Loss train: 0.296331  [    0/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 0.289475\n",
            "\n",
            "Loss train: 0.234875  [ 1600/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 0.290852\n",
            "\n",
            "Loss train: 0.268220  [ 3200/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 0.296519\n",
            "\n",
            "Loss train: 0.258969  [ 4800/ 6295]\n",
            "Evaluating...\n",
            "Loss test: 0.287277\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = test_model(test_dataloader, eval=False)\n",
        "print(f'F1-score = {f1_score(y_pred, test[\"acceptable\"]):>3f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74_UlttYBQM2",
        "outputId": "4d210b52-3773-4575-afee-a80b1db7778b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score = 0.854427\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <td>Model</td>\n",
        "            <td>F1 score</td>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td>ruBERT-base</td>\n",
        "            <td>0.86</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>ruGPT3-large best zero shot</td>\n",
        "            <td>0.85</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>ruGPT3-large best few shot</td>\n",
        "            <td>0.85</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>ruT5-base</td>\n",
        "            <td>0.85</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "eZn_CebhdIhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модели показали схожий результат, однако я бы отдал предпочтение методу few-shot, потому что там почти ничего обучать не надо)"
      ],
      "metadata": {
        "id": "kaohxRJpd-U5"
      }
    }
  ]
}