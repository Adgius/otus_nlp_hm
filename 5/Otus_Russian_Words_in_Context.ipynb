{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84pgOCxu2_r"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import get_scheduler\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "DN7yJVVavKER"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://russiansuperglue.com/tasks/download/RUSSE"
      ],
      "metadata": {
        "id": "8uhRnf2yvNaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip RUSSE -d WiC"
      ],
      "metadata": {
        "id": "jHaj6pmrv0HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_json('/content/WiC/RUSSE/train.jsonl', orient='records', lines = True)\n",
        "val = pd.read_json('/content/WiC/RUSSE/val.jsonl', orient='records', lines = True)\n",
        "test = pd.read_json('/content/WiC/RUSSE/test.jsonl', orient='records', lines = True)\n",
        "\n",
        "print('Train size:', len(train))\n",
        "print('Val size:', len(val))\n",
        "print('Test size:', len(test))\n",
        "print('\\n')\n",
        "print('Train labels counts\\n', train['label'].value_counts().to_dict(), '\\n')\n",
        "print('Eval labels counts\\n', val['label'].value_counts().to_dict(), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALqAMvJRwot_",
        "outputId": "59b87697-a293-4c61-f753-9e2781349983"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 19845\n",
            "Val size: 8505\n",
            "Test size: 18892\n",
            "\n",
            "\n",
            "Train labels counts\n",
            " {False: 12653, True: 7192} \n",
            "\n",
            "Eval labels counts\n",
            " {False: 5366, True: 3139} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "ms1HR5z_0eY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
        "model = AutoModel.from_pretrained(\"ai-forever/ruBert-base\", output_hidden_states = True)"
      ],
      "metadata": {
        "id": "Pi3YDso10bmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in pd.concat([train['sentence1'], train['sentence2']]):\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GyTBR0Q1WaA",
        "outputId": "19a0245d-bd8b-470b-9b03-bd0576a5f7b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_ids_to_mask(text, start, end, max_length=False, debug=False):\n",
        "    text = text.replace('й', 'и').replace('ё', 'е')\n",
        "    input_ids = tokenizer(text, return_tensors='pt')['input_ids']\n",
        "    n = 0\n",
        "    symbols = []\n",
        "    mask = []\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.reshape(-1))\n",
        "    for t in range(len(tokens)):\n",
        "        if t == 0 :\n",
        "            symbols.append([-1])\n",
        "            continue\n",
        "        elif t == len(tokens)-1:\n",
        "            symbols.append([-1])\n",
        "            break\n",
        "        symbol = []\n",
        "        for s in tokens[t].replace('#', \"\"):\n",
        "            if text.lower()[n] == s:\n",
        "                symbol.append(n)\n",
        "                n += 1\n",
        "        symbols.append(symbol)\n",
        "        if n != len(text):\n",
        "            # В тексте есть символы #, из-за этого может в ошибку упасть\n",
        "            try: \n",
        "                if text.lower()[n] != tokens[t+1].replace('#', \"\")[0]:\n",
        "                    n += 1\n",
        "            except:\n",
        "                n += 1\n",
        "    for s in symbols:\n",
        "        if set(s) & set(range(start, end)):\n",
        "            mask.append(1.)\n",
        "        else:\n",
        "            mask.append(0.)\n",
        "    if debug:\n",
        "        for t, m in zip(tokens, mask):\n",
        "            print(f'{t:<15} {m}')\n",
        "    else:\n",
        "        if max_length:\n",
        "            if max_length > len(mask):\n",
        "                mask.extend([0]*(max_length - len(mask)))\n",
        "            else:\n",
        "                mask = mask[:max_length]\n",
        "        return mask\n",
        "\n",
        "convert_ids_to_mask('Солнце стояло уже высоко, когда справа от дороги я увидел деревеньку дворов в пятнадцать', 57, 68, debug=True)"
      ],
      "metadata": {
        "id": "L6jBo4lq8xlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8068d001-af06-4b98-8ffd-2626b03425ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]           0.0\n",
            "солнце          0.0\n",
            "стояло          0.0\n",
            "уже             0.0\n",
            "высоко          0.0\n",
            ",               0.0\n",
            "когда           0.0\n",
            "справа          0.0\n",
            "от              0.0\n",
            "дороги          0.0\n",
            "я               0.0\n",
            "увидел          0.0\n",
            "деревень        1.0\n",
            "##ку            1.0\n",
            "дворов          0.0\n",
            "в               0.0\n",
            "пятнадцать      0.0\n",
            "[SEP]           0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True).drop(columns='idx')\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=71)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index, :]\n",
        "        input_ids_1, _, attention_mask_1 = self.tokenize(row['sentence1']).values()\n",
        "        input_ids_2, _, attention_mask_2 = self.tokenize(row['sentence2']).values()\n",
        "        mask1 = convert_ids_to_mask(row['sentence1'], row['start1'], row['end1'], max_length=71)\n",
        "        mask2 = convert_ids_to_mask(row['sentence2'], row['start2'], row['end2'], max_length=71)\n",
        "        label = [1.] if row['label'] else [-1.]\n",
        "        output = {\n",
        "            'input_ids_1': input_ids_1.reshape(-1),\n",
        "            'attention_mask_1': attention_mask_1.reshape(-1),\n",
        "            'input_ids_2': input_ids_2.reshape(-1),\n",
        "            'attention_mask_2': attention_mask_2.reshape(-1),\n",
        "            'word1_mask': torch.tensor(mask1),\n",
        "            'word2_mask': torch.tensor(mask2),\n",
        "            'labels': torch.tensor(label),\n",
        "        }\n",
        "        return {k: v.to(device) for k, v in output.items()}\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        return tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=71)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index, :]\n",
        "        input_ids_1, _, attention_mask_1 = self.tokenize(row['sentence1']).values()\n",
        "        input_ids_2, _, attention_mask_2 = self.tokenize(row['sentence2']).values()\n",
        "        mask1 = convert_ids_to_mask(row['sentence1'], row['start1'], row['end1'], max_length=71)\n",
        "        mask2 = convert_ids_to_mask(row['sentence2'], row['start2'], row['end2'], max_length=71)\n",
        "        output = {\n",
        "            'input_ids_1': input_ids_1.reshape(-1),\n",
        "            'attention_mask_1': attention_mask_1.reshape(-1),\n",
        "            'input_ids_2': input_ids_2.reshape(-1),\n",
        "            'attention_mask_2': attention_mask_2.reshape(-1),\n",
        "            'word1_mask': torch.tensor(mask1),\n",
        "            'word2_mask': torch.tensor(mask2),\n",
        "        }\n",
        "        return {k: v.to(device) for k, v in output.items()}\n",
        "        \n",
        "\n",
        "train_ds = TrainDataset(train)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "\n",
        "eval_ds = TrainDataset(val)\n",
        "eval_dataloader = DataLoader(eval_ds, batch_size=16)\n",
        "\n",
        "test_ds = TestDataset(test)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=16)"
      ],
      "metadata": {
        "id": "7pCuBP3s5P1g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WiC_Head(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    original: https://github.com/llightts/CSI5138_Project/blob/master/RoBERTa_WiC_baseline.ipynb\n",
        "    \n",
        "    Логика:\n",
        "    1. Находим эмбеддинг токенов таргетных слов\n",
        "    2. Суммируем все слои\n",
        "    3. Считаем разницу\n",
        "    4. Пропускаем через несколько полносвязных слоёв\n",
        "    5. Получаем скор схожести\n",
        "    \"\"\"\n",
        "    def __init__(self, model, embedding_size = 768):\n",
        "        \"\"\"\n",
        "        Keeps a reference to the provided RoBERTa model. \n",
        "        It then adds a linear layer that takes the distance between two \n",
        "        \"\"\"\n",
        "        super(WiC_Head, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.embedder = model\n",
        "        self.linear_1 = torch.nn.Linear(3*embedding_size, embedding_size, bias = True)\n",
        "        self.dropout = torch.nn.Dropout1d(p=0.1)\n",
        "        self.linear_2 = torch.nn.Linear(embedding_size, 1, bias = True)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def hinge_loss(self, y_pred, y_true):\n",
        "        return torch.mean(torch.clamp(1 - y_pred * y_true, min=0)) \n",
        "\n",
        "    def forward(self, input_ids_1=None, input_ids_2=None, attention_mask_1=None, attention_mask_2=None, \n",
        "                labels=None, word1_mask = None, word2_mask = None, **kwargs):\n",
        "        # Get the embeddings\n",
        "        batch_size = word1_mask.shape[0]\n",
        "\n",
        "        _, _, hidden_states_1 = self.embedder(input_ids=input_ids_1, attention_mask=attention_mask_1).values()\n",
        "        _, _, hidden_states_2 = self.embedder(input_ids=input_ids_2, attention_mask=attention_mask_2).values()\n",
        "\n",
        "        words1_hidden_states = [torch.bmm(word1_mask.unsqueeze(1), h).view(batch_size, self.embedding_size) for h in hidden_states_1]\n",
        "        words2_hidden_states = [torch.bmm(word2_mask.unsqueeze(1), h).view(batch_size, self.embedding_size) for h in hidden_states_2]\n",
        "\n",
        "        word1_emb = torch.zeros(batch_size, self.embedding_size, device=device)\n",
        "        word2_emb = torch.zeros(batch_size, self.embedding_size, device=device)\n",
        "\n",
        "        for w1, w2 in zip(words1_hidden_states, words2_hidden_states):\n",
        "            word1_emb += w1\n",
        "            word2_emb += w2\n",
        "\n",
        "        diff = word1_emb - word2_emb\n",
        "        \n",
        "        word_emb = torch.concat([word1_emb, word1_emb, diff], axis=1)\n",
        "\n",
        "        # Calculate outputs using activation\n",
        "        layer1_results = self.activation(self.dropout(self.linear_1(word_emb)))\n",
        "        layer2_results = self.activation(self.linear_2(layer1_results))\n",
        "        logits = torch.tanh(layer2_results).view(-1)\n",
        "\n",
        "        outputs = logits\n",
        "        # Calculate the loss\n",
        "        if labels is not None:\n",
        "            #  We want seperation like a SVM so use Hinge loss\n",
        "            loss = self.hinge_loss(logits, labels.view(-1))\n",
        "            outputs = (loss, logits)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "YWtC2wfU7b2X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wic_model = WiC_Head(model)"
      ],
      "metadata": {
        "id": "wr-kty1gwW8w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)\n",
        "wic_model.to(device);"
      ],
      "metadata": {
        "id": "b6rUx6j545P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(wic_model.parameters(), lr=5e-6)\n",
        "\n",
        "num_epochs = 2\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "T5DIrahcu63C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [-1, 1] -> [0, 1]\n",
        "def scale(y_pred):\n",
        "    return (y_pred + 1)/2"
      ],
      "metadata": {
        "id": "sMw2XYmzC3Td"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dataloader, num_epochs):\n",
        "    wic_model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1} \\n -------------------')\n",
        "        for n_batch, batch in enumerate(train_dataloader):\n",
        "            loss, logits = wic_model(**batch)\n",
        "            if n_batch % 200 == 0:\n",
        "                loss_value, current = loss.item(), n_batch * batch['input_ids_1'].shape[0]\n",
        "                print(f\"Loss train: {loss_value:>7f}  [{current:>5d}/{len(train_ds):>5d}]\")\n",
        "                print('Evaluating...')\n",
        "                preds, true = test_model(eval_dataloader, eval=True)\n",
        "                preds = scale(preds)\n",
        "                true = scale(preds)\n",
        "                print(f'Accuracy_score = {accuracy_score(preds > 0.5, true):>3f}\\n')\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad() \n",
        "\n",
        "def test_model(test_dataloader, eval=False):\n",
        "    wic_model.eval()\n",
        "    y_pred = np.array([])\n",
        "    y_true = np.array([])\n",
        "    for n_batch, batch in enumerate(test_dataloader):\n",
        "        if eval:\n",
        "            y_true = np.hstack([y_true, batch['labels'].cpu().numpy().reshape(-1)])\n",
        "            _, logits = wic_model(**batch)\n",
        "        else:\n",
        "            logits = wic_model(**batch)\n",
        "        y_pred = np.hstack([y_pred, logits.detach().cpu().numpy()])\n",
        "    return y_pred, y_true"
      ],
      "metadata": {
        "id": "yP3Em69Vu4Oz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_dataloader, num_epochs)"
      ],
      "metadata": {
        "id": "G7SnAYD_yj8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_logits, _ = test_model(test_dataloader, eval=False)"
      ],
      "metadata": {
        "id": "-YKeBJhxwh3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = [\"true\" if float(i) > 0 else \"false\"  for i in test_logits]\n",
        "output = [f'{{\"idx\": {n}, \"label\": \"{i}\"}}' for n, i in enumerate(output)]"
      ],
      "metadata": {
        "id": "fjxjvdd0Qrv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('RUSSE.jsonl', 'w') as f:\n",
        "    f.writelines('\\n'.join(output))"
      ],
      "metadata": {
        "id": "lLKeH9GpW9vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr8-jOOGRcTk",
        "outputId": "7a4a49d6-9bc9-4575-98ed-2e9fd8834534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('drive/MyDrive/OTUS/wic')\n",
        "tokenizer.save_pretrained('drive/MyDrive/OTUS/wic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXjZzbwgRXr7",
        "outputId": "5697d3e6-0c28-4300-b2ba-a9fdd253aa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/OTUS/wic/tokenizer_config.json',\n",
              " 'drive/MyDrive/OTUS/wic/special_tokens_map.json',\n",
              " 'drive/MyDrive/OTUS/wic/vocab.txt',\n",
              " 'drive/MyDrive/OTUS/wic/added_tokens.json',\n",
              " 'drive/MyDrive/OTUS/wic/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(wic_model.state_dict(), 'drive/MyDrive/OTUS/wic/wic_model.pt')"
      ],
      "metadata": {
        "id": "1je6DIl0CsDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}